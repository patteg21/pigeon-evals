task: "example_eval_task"
dataset: 
  path: "data/processed_filings/META"

threading:
  max_workers: 8

preprocess:
  ocr: "easyocr"

parser:
  type: "multistage"
  processes:
    - name: "table_extraction"
      steps:
        - strategy: "regex"
          regex_pattern: "\\[TABLE_START\\][\\s\\S]*?\\[TABLE_END\\]"
          ignore_case: true

    - name: "paragraph_extraction"
      steps:
        - strategy: "regex"
          regex_pattern: "\\[PAGE BREAK\\]"
          ignore_case: true
        - strategy: "paragraph"
          chunk_size: null
          chunk_overlap: 256 


embedding:
  provider: "huggingface"
  model: "sentence-transformers/all-MiniLM-L6-v2"
  pooling_strategy: "mean"
  dimension_reduction:
    type: "PCA"
    dims: 256
  use_threading: true


storage:
  text_store:
    client: "sqlite"
    path: "./data/text_store.db"
    upload: true

    # PostgreSQL options (commented out):
    # host: "localhost"
    # port: 5432
    # database: "pigeon_evals"
    # user: "postgres"
    # password: ""

    # S3 options (commented out):
    # bucket_name: "pigeon-evals-documents"
    # prefix: "documents/"
    # access_key_id: null
    # secret_access_key: null
    # region: "us-east-1"
    
    # File store options (commented out):
    # base_path: "data/documents"
  vector:
    provider: "faiss"  # Added explicit provider
    path: "./data/.faiss/index"
    upload: true
    
    clear: false
    index: "eval_index"
    dimension: 256  # Added to match embedding dimension_reduction
    # Alternative vector options (commented out):
    # index_name: "alternative_index"
  outputs:
    - "chunks"
    - "documents"

eval:
  top_k: 10
  provider: "openai"
  model: "gpt-4o"
  evaluations: true
  metrics:
    - "ndcg"
    - "precision"
    - "recall"
  
  rerank:
    provider: "huggingface"
    model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
    top_k: 5
  
  test:
    load:
      path: "data/tests/default.json"
      key: "tests"
      
    tests:
      - type: "llm"
        name: "basic_llm_test"
        query: "What is the main topic?"
        prompt: "Analyze the retrieved documents and provide a summary."
        eval_type:
          - "single"
      
      - type: "human"
        name: "human_review"
        query: "Quality assessment query"
      
      - type: "agent"
        name: "agent_test"
        query: "Test agent functionality"
        prompt: "Execute the agent task"
        mcp:
          command: "python"
          args:
            - "agent_script.py"
            - "--test"